{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3e1ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df56f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "input_dim = 784\n",
    "hidden_dim = 256\n",
    "latent_dim = 2\n",
    "epochs = 50\n",
    "epsilon_std = 1.0\n",
    "\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f135ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "    super(Net, self).__init__()\n",
    "    input_dim = torch.tensor((input_dim, ))\n",
    "    self.enc1 = nn.Linear(input_dim, hidden_dim)\n",
    "    self.enc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "    self.enc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    self.dec1 = nn.Linear(latent_dim, hidden_dim)\n",
    "    self.dec2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "  def reparameterize(self, mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.rand_like(std)\n",
    "    sample = mu + (eps * std)\n",
    "    return sample\n",
    "        \n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.enc1(x))\n",
    "    mu = self.enc21(x)\n",
    "    logvar = self.enc22(x)\n",
    "\n",
    "    z = self.reparameterize(mu, logvar)\n",
    "\n",
    "    x = F.relu(self.dec1(z))\n",
    "    x = self.dec2(x)\n",
    "    reconstruction = torch.sigmoid(x)\n",
    "    return reconstruction, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd887c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(input_dim, hidden_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c3ddf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(reconstruction, data, mu, log_var):\n",
    "    BCE = nn.BCELoss(reduction='sum')(reconstruction, data)\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e4377ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2e788a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# transformer\n",
    "transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "# train and validation data\n",
    "train_data = datasets.MNIST(\n",
    "    root='input/data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# training data loader\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_data = datasets.MNIST(\n",
    "    root='input/data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "586dc543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_message(log, precision=4):\n",
    "  fmt = \"{0}: {1:.\" + str(precision) + \"f}\"\n",
    "  return \" | \".join(fmt.format(k, v) for k, v in log.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "079a4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressBar():\n",
    "  def __init__(self, n, length=40):\n",
    "    # protect against division by zero\n",
    "    self.n = max(1, n)\n",
    "    self.nf = float(n)\n",
    "    self.length = length\n",
    "    # precalculate the i values that should trigger a write operation\n",
    "    self.ticks = set([round(i / 100.0 * n) for i in range(101)])\n",
    "    self.ticks.add(n - 1)\n",
    "  \n",
    "  def bar(self, i, message=\"\"):\n",
    "    \"\"\"Assumes i ranges through [0, n - 1]\"\"\"\n",
    "    if i in self.ticks:\n",
    "      b = int(np.ceil(((i + 1) / self.nf) * self.length))\n",
    "      sys.stdout.write(\"\\r[{0}{1}] {2}%\\t{3}\".format(\n",
    "        \"=\"*b, \" \" * (self.length - b), int(100 * ((i + 1) / self.nf)), message))\n",
    "      sys.stdout.flush()\n",
    "  \n",
    "  def close(self, message=\"\"):\n",
    "    \"\"\"Move the bar to 100% before closing\"\"\"\n",
    "    self.bar(self.n-1)\n",
    "    sys.stdout.write(\"{0}\\n\".format(message))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1fca02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "[========================================] 100%\tTrain Loss: 246.9296 | Validation Loss: 195.6259\n",
      "Epoch 2/50\n",
      "[========================================] 100%\tTrain Loss: 189.2877 | Validation Loss: 183.8044\n",
      "Epoch 3/50\n",
      "[========================================] 100%\tTrain Loss: 181.6288 | Validation Loss: 178.8063\n",
      "Epoch 4/50\n",
      "[========================================] 100%\tTrain Loss: 177.5929 | Validation Loss: 175.3893\n",
      "Epoch 5/50\n",
      "[========================================] 100%\tTrain Loss: 174.5516 | Validation Loss: 172.7282\n",
      "Epoch 6/50\n",
      "[========================================] 100%\tTrain Loss: 171.9260 | Validation Loss: 170.3677\n",
      "Epoch 7/50\n",
      "[========================================] 100%\tTrain Loss: 169.7205 | Validation Loss: 168.6553\n",
      "Epoch 8/50\n",
      "[========================================] 100%\tTrain Loss: 167.9025 | Validation Loss: 166.9637\n",
      "Epoch 9/50\n",
      "[========================================] 100%\tTrain Loss: 166.3852 | Validation Loss: 165.6461\n",
      "Epoch 10/50\n",
      "[========================================] 100%\tTrain Loss: 165.1771 | Validation Loss: 164.6793\n",
      "Epoch 11/50\n",
      "[========================================] 100%\tTrain Loss: 164.1399 | Validation Loss: 163.7516\n",
      "Epoch 12/50\n",
      "[========================================] 100%\tTrain Loss: 163.2601 | Validation Loss: 162.9131\n",
      "Epoch 13/50\n",
      "[========================================] 100%\tTrain Loss: 162.4579 | Validation Loss: 162.0123\n",
      "Epoch 14/50\n",
      "[========================================] 100%\tTrain Loss: 161.7551 | Validation Loss: 161.4527\n",
      "Epoch 15/50\n",
      "[========================================] 100%\tTrain Loss: 161.0938 | Validation Loss: 160.8102\n",
      "Epoch 16/50\n",
      "[========================================] 100%\tTrain Loss: 160.5172 | Validation Loss: 160.2841\n",
      "Epoch 17/50\n",
      "[========================================] 100%\tTrain Loss: 159.9727 | Validation Loss: 159.8580\n",
      "Epoch 18/50\n",
      "[========================================] 100%\tTrain Loss: 159.4679 | Validation Loss: 159.2138\n",
      "Epoch 19/50\n",
      "[========================================] 100%\tTrain Loss: 158.9965 | Validation Loss: 158.8536\n",
      "Epoch 20/50\n",
      "[========================================] 100%\tTrain Loss: 158.5866 | Validation Loss: 158.4995\n",
      "Epoch 21/50\n",
      "[========================================] 100%\tTrain Loss: 158.1831 | Validation Loss: 158.0873\n",
      "Epoch 22/50\n",
      "[========================================] 100%\tTrain Loss: 157.8108 | Validation Loss: 157.7381\n",
      "Epoch 23/50\n",
      "[========================================] 100%\tTrain Loss: 157.4629 | Validation Loss: 157.4213\n",
      "Epoch 24/50\n",
      "[========================================] 100%\tTrain Loss: 157.1342 | Validation Loss: 157.2381\n",
      "Epoch 25/50\n",
      "[========================================] 100%\tTrain Loss: 156.8056 | Validation Loss: 156.8152\n",
      "Epoch 26/50\n",
      "[========================================] 100%\tTrain Loss: 156.5170 | Validation Loss: 156.5628\n",
      "Epoch 27/50\n",
      "[========================================] 100%\tTrain Loss: 156.2251 | Validation Loss: 156.2520\n",
      "Epoch 28/50\n",
      "[========================================] 100%\tTrain Loss: 155.9574 | Validation Loss: 156.0419\n",
      "Epoch 29/50\n",
      "[========================================] 100%\tTrain Loss: 155.7039 | Validation Loss: 155.7791\n",
      "Epoch 30/50\n",
      "[========================================] 100%\tTrain Loss: 155.4485 | Validation Loss: 155.4965\n",
      "Epoch 31/50\n",
      "[========================================] 100%\tTrain Loss: 155.2229 | Validation Loss: 155.3357\n",
      "Epoch 32/50\n",
      "[========================================] 100%\tTrain Loss: 155.0033 | Validation Loss: 155.1210\n",
      "Epoch 33/50\n",
      "[========================================] 100%\tTrain Loss: 154.7851 | Validation Loss: 155.1221\n",
      "Epoch 34/50\n",
      "[========================================] 100%\tTrain Loss: 154.5792 | Validation Loss: 154.8970\n",
      "Epoch 35/50\n",
      "[========================================] 100%\tTrain Loss: 154.3922 | Validation Loss: 154.6171\n",
      "Epoch 36/50\n",
      "[========================================] 100%\tTrain Loss: 154.1858 | Validation Loss: 154.5292\n",
      "Epoch 37/50\n",
      "[========================================] 100%\tTrain Loss: 154.0406 | Validation Loss: 154.2415\n",
      "Epoch 38/50\n",
      "[========================================] 100%\tTrain Loss: 153.8516 | Validation Loss: 154.2700\n",
      "Epoch 39/50\n",
      "[========================================] 100%\tTrain Loss: 153.6666 | Validation Loss: 154.0354\n",
      "Epoch 40/50\n",
      "[========================================] 100%\tTrain Loss: 153.5158 | Validation Loss: 154.0174\n",
      "Epoch 41/50\n",
      "[========================================] 100%\tTrain Loss: 153.3765 | Validation Loss: 153.7761\n",
      "Epoch 42/50\n",
      "[========================================] 100%\tTrain Loss: 153.2202 | Validation Loss: 153.6329\n",
      "Epoch 43/50\n",
      "[========================================] 100%\tTrain Loss: 153.0654 | Validation Loss: 153.5139\n",
      "Epoch 44/50\n",
      "[========================================] 100%\tTrain Loss: 152.9273 | Validation Loss: 153.3938\n",
      "Epoch 45/50\n",
      "[========================================] 100%\tTrain Loss: 152.7766 | Validation Loss: 153.2555\n",
      "Epoch 46/50\n",
      "[========================================] 100%\tTrain Loss: 152.6322 | Validation Loss: 153.2828\n",
      "Epoch 47/50\n",
      "[========================================] 100%\tTrain Loss: 152.4978 | Validation Loss: 153.0774\n",
      "Epoch 48/50\n",
      "[========================================] 100%\tTrain Loss: 152.3828 | Validation Loss: 152.9574\n",
      "Epoch 49/50\n",
      "[========================================] 100%\tTrain Loss: 152.2742 | Validation Loss: 152.9006\n",
      "Epoch 50/50\n",
      "[========================================] 100%\tTrain Loss: 152.1176 | Validation Loss: 152.8026\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict \n",
    "p = ProgressBar(len(train_loader))\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  log = OrderedDict()\n",
    "  print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "  running_loss = 0.0\n",
    "  # Train\n",
    "  for i, data in enumerate(train_loader):\n",
    "      data, _ = data\n",
    "      data = data.view(data.size(0), -1)\n",
    "      opt.zero_grad()\n",
    "      reconstruction, mu, log_var = model(data)\n",
    "      loss = final_loss(reconstruction, data, mu, log_var)\n",
    "      running_loss += loss.item()\n",
    "      p.bar(i)\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "  log['Train Loss'] = running_loss / len(train_loader.dataset)\n",
    "  p.bar(i)\n",
    "  \n",
    "  # validate\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  with torch.no_grad():\n",
    "    for i, data in enumerate(val_loader):\n",
    "        data, _ = data\n",
    "        data = data.view(data.size(0), -1)\n",
    "        reconstruction, mu, log_var = model(data)\n",
    "        loss = final_loss(reconstruction, data, mu, log_var)\n",
    "        running_loss += loss.item()\n",
    "    log['Validation Loss'] = running_loss / len(val_loader.dataset)\n",
    "  p.close(log_to_message(log))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
